### asyncio的基本使用

包含各种特定系统的模块化事件循环

传输和协议抽象

对ＴＣＰ、ＵＤＰ、ＳＳＬ、子进程延时调用以及其他的具体支持

模仿futures模块但适用于事件循环使用的Ｆｕｔｕｒｅ类

基于yield　from 的协议和任务，可以让你使用顺序方法编写并发代码

必须使用一个将产生阻塞ＩＯ调用时，有接口可以把这个事件转移到线程池

模仿threading模块中的同步原语，可以用在单线程内的协成之间

#### 入门案例

```python
import asyncio
import time

async def get_html(url):
    print("strt asyncio ")
    await asyncio.sleep(5)
    print("end asyncio")
if __name__ == "__main__":
    loop = asyncio.get_event_loop()   # 循环监听
    start = time.time()
    tasks = [get_html(str(i)) for i in range(10)]
    loop.run_until_complete(asyncio.wait(tasks))　　# 执行内容　asyncio.wait()可以放列表（多个）
    print(time.time() - start)
```

#### 获取返回值

```python
import asyncio
import time

async def get_html(url):
    print("strt asyncio ")
    await asyncio.sleep(2)
    print("end asyncio")
    return url
def callback(url):
    print(url.result,"email")
    
if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    start = time.time()
    # 第一种　获取返回值的方式
    # get_future = asyncio.ensure_future(get_html(666))　
    # loop.run_until_complete(get_future)
    # print(get_future.result())
    # 第二种获取返回值的方式
    task = loop.create_task(get_html(777))
    task.add_done_callback(callback)   # 添加回调函数
    loop.run_until_complete(task)
    print(task.result())
    print(time.time() - start)
```

#### await 和gather的区别

```python
import asyncio
import time

async def get_html(url):
    print("strt asyncio ")
    await asyncio.sleep(5)
    print("end asyncio")
if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    start = time.time()
    task1 = [get_html(str(i)) for i in range(1,4)]
    task2 = [get_html(str(i)) for i in range(10,12)]
    task1 = asyncio.gather(*task1)
    task2 = asyncio.gather(*task2)
    # 取消任务
    task2.cancle()
	# 第一种：gather的使用
    loop.run_until_complete(asyncio.gather(task1,task2))
    # 第二种：wait的使用
    loop.run_until_complete(asyncio.wait(tasks))
    # 区别：gather比ｗａｉｔ更加高级

    
    print(time.time() - start)
```

#### call_soon  call_later  call_at  call_soon_threadsafe的使用

```python
import asyncio


def callback(sleep_time):
    print(f"sleep {sleep_time} success")


def stop_loop(loop):
    loop.stop()


# call_soon  call_later  call_at  call_soon_threadsafe

if __name__ == "__main__":

    loop = asyncio.get_event_loop()
    loop.call_soon(callback, 4)		　# call_soon 即刻执行，在下一个循环（回调函数，时间）
    loop.call_later(2, callback, 2)   # call_later　在指定时间执行（时间（传统时间），回调函数，参数）
    loop.call_later(1, callback, 1)
    loop.call_later(3, callback, 3)
    
    new = asyncio.time()			　　　# 获取异步的时间
    loop.call_at(new + 1, callback)		# 时间（异步的时间）
    loop.call_soon_threadsafe()　　　　　 # 多了线程安全

    # loop.call_soon(stop_loop,loop)
    loop.run_forever()					# 一直执行，不停止

```

#### 多线程启动异步

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time
# asyncio 与线程池结合

def callback(url):
    time.sleep(2)
    print(url)

if __name__ == "__main__":
    start = time.time()
    loop = asyncio.get_event_loop()
    # 实例化线程池
    executor = ThreadPoolExecutor()
    tasks = []
    for i in range(10):
        task = loop.run_in_executor(executor,callback,i)
        tasks.append(task)
    
    loop.run_until_complete(asyncio.wait(tasks))
    print(time.time()- start)
```

#### 异步的ＨＴＴＰ请求

```python
# asyncio 没有提供http协议接口 aiohttp
import asyncio
from urllib.parse import urlparse
import time
from asyncio import Lock

async def get_html(url):
    url = urlparse(url)
    host = url.netloc
    path = url.path
    # print(host,path)
    if path == "":
        path = "/"
    reader, write = await asyncio.open_connection(host, 80)
    write.write(
        "GET {} HTTP/1.1\r\nHost:{}\r\nConnection:close\r\n\r\n".format(path, host).encode("utf-8"))
    all_lines = []
    async for raw_line in reader:
        data = raw_line.decode("utf-8")
        all_lines.append(data)
    html = "\n".join(all_lines)
    return html


async def main():
    tasks = []
    for i in range(20):
        url = "http://shop.projectsedu.com/goods/{}/".format(i)
        tasks.append(asyncio.ensure_future(get_html(url)))

    for task in asyncio.as_completed(tasks):
        result = await task
        print(result)
if __name__ == "__main__":
    start = time.time()
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
    print(time.time() - start)

```

#### 异步爬虫实例

```python
# asyncio 爬虫 去重 入库 aiomysql
import re
import asyncio

import aiohttp
import aiomysql
from aiomysql import create_pool

from pyquery import PyQuery

stoping = False
satrt_url = "http://www.jobbole.com/"
waitting_urls = []
seen_url = set()

# 设置并发
sem = asyncio.Semaphore(1)

async def featch(url, session):
    async with sem:
        await asyncio.sleep(1)
        try:
            async with session.get(url) as resp:
                print("url status {}".format(resp.status))
                if resp.status in [200, 201]:
                    data = await resp.text()
                    return data
        except Exception as e:
            print(e)


def extract_urls(html):
    urls = []
    pq = PyQuery(html)
    for link in pq.items('a'):
        url = link.attr("href")
        if url and url.startswith("http") and url not in seen_url:
            urls.append(url)
            waitting_urls.append(url)
    return urls


async def arcitle_handler(url, session, pool):
    # 获取文章详情并解析入库
    html = await featch(url, session)
    seen_url.add(url)
    extract_urls(html)
    pq = PyQuery(html)
    title = pq("title").text()
    async with pool.acquire() as conn:
        async with conn.cursor() as cur:
            insert_sql = "insert into user(name) values ('{}')".format(title)
            await cur.execute(insert_sql)


async def init_url(url, session):
    html = await featch(url, session)
    seen_url.add(url)
    extract_urls(html)


async def consumer(pool):
    async with aiohttp.ClientSession() as session:
        while not stoping:
            if len(waitting_urls) == 0:
                await asyncio.sleep(0.5)
                continue
            url = waitting_urls.pop()
            print("start get url {}".format(url))
            if re.match("http://.*?jobbole.com/\\d+/", url):
                if url not in seen_url:
                    asyncio.ensure_future(arcitle_handler(url, session, pool))
                    await asyncio.sleep(0.5)
            else:
                if url not in seen_url:
                    asyncio.ensure_future(init_url(url, session))


async def main(loop):
    # 先链接好mysql
    pool = await create_pool(
        host='127.0.0.1', port=3306,
        user='root', password='123456',
        db='hello', loop=loop,
        charset="utf8", autocommit=True)
    async with aiohttp.ClientSession() as session:
        html = await featch(satrt_url, session)
        seen_url.add(satrt_url)
        extract_urls(html)

    asyncio.ensure_future(consumer(pool))


if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    asyncio.ensure_future(main(loop))
    loop.run_forever()

```



