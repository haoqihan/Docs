### 无监督学习的目标

利用无标签的数据学习数据的分布或数据与数据之间的关系被称作为无监督学习

有监督学习与无监督学习最大的区别在于数据是否有标签

无监督学习最常用的场景是**聚类**和**降维**

### 聚类

聚类,就是根据数据的"相似性"将数据分为多类的过程

评估两个不同样本之间的相似性,通常使用的方法就是计算两个样本之间的距离,使用不同的方法计算样本间的距离会关系到聚类结果的好坏

#### 欧式距离

欧式距离是最常用的一种距离度量方法,源于欧式空间中两点之间的距离

#### 曼哈顿距离

曼哈顿距离也称作"城市街区距离",类似于在城市之中驾车行驶,从一个十字路口到另一个十字路口的距离

#### 马氏距离

马氏距离表示数据的协方差距离,是一种尺度无关的度量方式也就是说马氏距离会先将样本点的各个属性标准化,再计算样本间的距离

#### 夹角余弦

余弦相似度用向量空间中两个向量夹角的余弦作为衡量两个样本差异的大小,余弦越接近1,说明两个向量夹角越接近0度,表明两个向量越相似

#### sklearn.cluster（分类）

sklearn.cluster 模块提供的各聚类算法函数可以使用不同数据形式作为输入

- 相似性矩阵输入格式：即由【样本数目】定义的矩阵形式，矩阵中的每一个元素为两个样本的相似度，如DBSCAN，AffinityPropagation（紧邻传播算法）接收这种输入，如果以余弦相似度为例，则对角线元素全为1，矩阵每个元素的取值范围为【0,1】

| 算法名称                          | 参数                     | 可扩展性                       | 相似度量         |
| --------------------------------- | ------------------------ | ------------------------------ | ---------------- |
| K-Means                           | 聚类个数                 | 大规模数据                     | 点间距离         |
| DBSCAN                            | 邻域大小                 | 大规模数据                     | 点间距离         |
| Gaussian Mixtures（高斯混合模型） | 聚类个数及其他超参       | 复杂度高，不适合出来大规模数据 | 马氏距离         |
| Birch                             | 分支因子，阈值等其他超参 | 大规模数据                     | 两点间的欧式距离 |

### 降维

降维,就是在保证数据所具有代表性特征或分布的情况下,将高维数据转化为低维数据的过程

- 数据可视化
- 精简数据

#### sklearn.decomposition

| 算法名称 | 参数               | 可扩展性     | 适用任务          |
| -------- | ------------------ | ------------ | ----------------- |
| PCA      | 所降维度和其他超参 | 大规模数据   | 信号处理          |
| FastICA  | 所降维度和其他超参 | 超大规模数据 | 图形图像特征提取  |
| NMF      | 所降维度和其他超参 | 大规模数据   | 图形图像特征提取  |
| LDA      | 所降维度和其他超参 | 大规模数据   | 文本数据,主题挖掘 |

### K-means聚类算法

k-means算法以k为参数,把N个对象分为k个簇,使簇内具有较高的相似度,而簇间的相似度较低

- 随机选择k个点作为初始的聚类中心
- 对于剩下的点,根据与聚类中心的距离,将其归为最近的簇
- 对每个簇,计算所有点的均值作为新的聚类中心
- 重复2,3直到聚类中心不再发生改变

#### DBSCAN密度聚类

##### DBSCAN算法是一种基于密度的聚类算法

- 聚类的时候不需要预先指定簇的个数
-  最终的簇的个数不定

##### DBSCAN算法将数据点分为三类

- 核心点：在半径Eps内含有超过MinPts数目的点
- 边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的邻域内
- 噪音点：既不是核心点也不是边界点的点

#### 主成分分析（PCA）

- 主成分分析是最常用的一种降维方法，通常用于高维数据集的探索与可视化，还可以用作数据压缩和处理等
- PCA可以把具有相关性的高维变量合称为线性无关的低维变量，成为主成分，主成分能够尽可能保留原始数据的信息

##### 方差

- 是各个样本和样本均值的差的平方和的均值，用来度量一组数据的分散程度

##### 协方差

- 用于度量两个变量之间的线性相关性程度，若两个变量的协方差为0，则可认为两者线性无关，协方差矩阵由变量的协方差值构成的矩阵（对称阵）

##### 特征向量

- 矩阵的特征向量是描述数据集结构的非零向量，并满足如下公式

##### 原理

- 矩阵的主成分就是协方差矩阵对应的特征向量，按照对应的特征值大小进行排序，最大的特征值就是第一主成分，其次是第二主成分，以此类推

#### sklearn中的主成分分析

- 在sklearn库中，可以使用sklearn.decomposition.PCA加载PCA进行降维，主要参数有
  - n_components : 指定主成分的个数，即降维后数据的维度
  - svd_solver: 设置特征值分解的方法，默认为“auto” 其他可选的有“full”、“arpack”、“randomized”

#### 非负矩阵分解（NMF）

- 非负矩阵分解（NMF）是在矩阵中所有元素均为非负数约束条件下的矩阵分解方法
- **基本思想**：给定一个非负矩阵V，NMF能够找到一个非负矩阵W和一个非负矩阵H，使得矩阵W和H的乘积近似等于矩阵V中的值
- W矩阵：基本图像矩阵，相当于原矩阵V中抽取出来的特征
- H矩阵：系数矩阵
- NMF能够广泛应用于图像分析、文本挖掘、和语音处理等领域

##### 矩阵分解优化目标

- 最小化W矩阵H矩阵的乘积和原始矩阵之间的差别
- 基于KL散度的优化目标

##### 导入

- 在sklearn库中，可以使用sklearn.decomposition.NMF 加载NMF算法，主要参数：
- n_components 用于指定分解后矩阵的单个维度K
- init： W矩阵和H矩阵的初始化方式，默认为“nndsvdar”

#### 图像分割

- **图像分割**：利用图像的灰度、颜色、纹理、形状等特征，把图像分成若
  干个互不重叠的区域，并使这些特征在同一区域内呈现相似性，在不同的区
  域之间存在明显的差异性。然后就可以将分割的图像中具有独特性质的区域
  提取出来用于不同的研究。
- 图像分割技术已在实际生活中得到广泛的应用。例如：在机车检验领域，
  可以应用到轮毂裂纹图像的分割，及时发现裂纹，保证行车安全；在生物医
  学工程方面，对肝脏CT图像进行分割，为临床治疗和病理学研究提供帮助。

#### 常见的分割常用方法

1. 阈值分割：对图像灰度值进行度量，设置不同类别的阈值，达到分割的目的。
2. 边缘分割：对图像边缘进行检测，即检测图像中灰度值发生跳变的地方，则为一片
   区域的边缘。
3. 直方图法：对图像的颜色建立直方图，而直方图的波峰波谷能够表示一块区域的颜
   色值的范围，来达到分割的目的。
4. 特定理论：基于聚类分析、小波变换等理论完成图像分割。



























